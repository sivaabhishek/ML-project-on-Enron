import sys
import pickle
import warnings
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectKBest
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import LinearSVC
from sklearn.ensemble import AdaBoostClassifier, VotingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import fbeta_score, make_scorer
from sklearn.model_selection import StratifiedKFold
from sklearn.pipeline import Pipeline

sys.path.append("../tools/")

from feature_format import featureFormat, targetFeatureSplit
from tester import dump_classifier_and_data

# Warning messages from GridSearchCV sometimes annoying
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)

### Task 1: Select what features you'll use.
### features_list is a list of strings, each of which is a feature name.
### The first feature must be "poi".

fin_features = ["salary", "deferral_payments", "total_payments",
                "loan_advances", "bonus", "restricted_stock_deferred",
                "deferred_income", "total_stock_value", "expenses",
                "exercised_stock_options", "other", "long_term_incentive",
                "restricted_stock", "director_fees"]

eml_features = ["to_messages", "from_poi_to_this_person", "from_messages",
                "from_this_person_to_poi", "shared_receipt_with_poi",
                "fraction_to_poi", "fraction_from_poi"]

# This is not the final features_list, we'll build it with SelectKBest
all_features = ["poi"] + fin_features + eml_features

sys.stdout.write("Loading dataset...   ")
sys.stdout.flush()

### Load the dictionary containing the dataset
with open("final_project_dataset.pkl", "rb") as data_file:
    data_dict = pickle.load(data_file)

sys.stdout.write("Done\n")

sys.stdout.write("Removing outliers...   ")
sys.stdout.flush()

### Task 2: Remove outliers
data_dict.pop("TOTAL")
data_dict.pop("THE TRAVEL AGENCY IN THE PARK")

sys.stdout.write("Done\n")

sys.stdout.write("Creating new features...   ")
sys.stdout.flush()

### Task 3: Create new feature(s)
### Store to my_dataset for easy export below.

# New features:
#     1. fraction_from_poi: from_poi_to_this_person / to_messages
#     2. fraction_to_poi: from_this_person_to_poi / from_messages

for point in data_dict.values():
    # Default values for those having missing values
    point.update({"fraction_from_poi": 0, "fraction_to_poi": 0})

    if "NaN" not in (point["from_poi_to_this_person"], point["to_messages"]):
        point["fraction_from_poi"] = \
            point["from_poi_to_this_person"] / point["to_messages"]

    if "NaN" not in (point["from_this_person_to_poi"], point["from_messages"]):
        point["fraction_to_poi"] = \
            point["from_this_person_to_poi"] / point["from_messages"]

sys.stdout.write("Done\n")

sys.stdout.write("Loading word data...   ")
sys.stdout.flush()

# Pre-built e-mail content dict generated by emails.py
with open("word_data.pkl", "rb") as f:
    word_data = pickle.load(f)

sys.stdout.write("Done\n")

sys.stdout.write("Vectorizing email data...   ")
sys.stdout.flush()

vectorizer = TfidfVectorizer(stop_words="english")
# Sort by names, make it compatible with featureFormat
tf = vectorizer.fit_transform([x[1] for x in sorted(word_data.items())])

word_features = vectorizer.get_feature_names()

sys.stdout.write("Done\n")

sys.stdout.write("Preprocessing data...   ")
sys.stdout.flush()

# Here I'm not adding all the word features into data_dict or my_dataset,
# that will waste too much time and they'll be extemely large. Instead,
# I preprocess the data_dict with featureFormat, then concatenate with
# the matrix generated by TfidfVectorizer.
# After that, do feature scaling and selection, then transform the
# final numpy array into original dict format
data = featureFormat(data_dict, all_features,
                     remove_all_zeroes=False, sort_keys=True)

# Concatenate two arrays vertically with np.hstack
data = np.hstack((data, tf.toarray()))

labels, features = targetFeatureSplit(data)

# Feature scaling
# Note that this applies to the final dataset used by tester.py,
# see L110~L115 and L178~L185
features = MinMaxScaler().fit_transform(features)

# Add an underscore before every word feature name to avoid ambiguity with
# original features
for feature in word_features:
    all_features.append("_" + feature)

sys.stdout.write("Done\n")

sys.stdout.write("Selecting features...   ")
sys.stdout.flush()

# Feature selection
selector = SelectKBest()

# Switch to True to perform grid search
search = False

if search:
    sys.stdout.write("\n")

    estimator = Pipeline([
        ("select", SelectKBest()),
        # Note that this SVM isn't the final model
        ("svm", LinearSVC()),
    ])
    # More than 20 is too much
    params = {"select__k": list(range(2, 20))}

    # Run 2 jobs at the same time, also print the progress into console
    # Here I use StratifiedKFold with 10 folds as CV for searching
    searcher = GridSearchCV(estimator, params, scoring="f1", n_jobs=2,
                            cv=StratifiedKFold(labels, 10), verbose=1)
    searcher.fit(features, labels)

    selector.k = searcher.best_params_["select__k"]

else:
    # The result I got is 9
    selector.k = 9

features = selector.fit_transform(features, labels)
# Get selected features using numpy array indexing
# all_features contains "poi" which isn't a feature
selected_features = np.array(all_features[1:])[selector.get_support()]

sys.stdout.write("Done\n")

sys.stdout.write("Generating final dataset...   ")
sys.stdout.flush()

# Generate my_dataset for testing script
my_dataset = {}
# Use sorting to make sure the features are mapping to the correct person
for name, point, label in zip(sorted(data_dict), features, labels):
    my_dataset[name] = {"poi": label}
    # point is an 1D array, each row of the 2D array features
    for feature, value in zip(selected_features, point):
        my_dataset[name][feature] = value

# Generate features_list for testing script
features_list = ["poi"] + list(selected_features)

sys.stdout.write("Done\n")

### Task 4: Try a varity of classifiers
### Please name your classifier clf for easy export below.
### Note that if you want to do PCA or other multi-stage operations,
### you'll need to use Pipelines. For more info:
### http://scikit-learn.org/stable/modules/pipeline.html

sys.stdout.write("Building model...   ")
sys.stdout.flush()

# C Parameter for this LinearSVC is tuned by hand
svm = LinearSVC(C=1.3)

nb = GaussianNB()

# Tune parameters in the next task
adabst = AdaBoostClassifier(DecisionTreeClassifier())

clf = VotingClassifier([
    ("svm", svm),
    ("nb", nb),
    ("ada", adabst),
])

sys.stdout.write("Done\n")

### Task 5: Tune your classifier to achieve better than .3 precision and recall
### using our testing script. Check the tester.py script in the final project
### folder for details on the evaluation method, especially the test_classifier
### function. Because of the small size of the dataset, the script uses
### stratified shuffle split cross validation. For more info:
### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html

sys.stdout.write("Tuning model...   ")
sys.stdout.flush()

# Switch to True to perform grid search
search = False

if search:

    sys.stdout.write("\n")

    # Here I use F2 score, which weights recall score more
    f2_score = make_scorer(fbeta_score, beta=2)

    param_grid = {
        "base_estimator__max_features": [None, "sqrt", "log2"],
        "base_estimator__max_depth": [None, 3, 5, 8, 10],
        "base_estimator__min_samples_leaf": [1, 2, 3, 5, 8],
        "learning_rate": [0.01, 0.1, 0.5, 0.8, 1.0],
    }

    searcher = GridSearchCV(adabst, param_grid, f2_score, n_jobs=2, verbose=1,
                            cv=StratifiedKFold(labels, 10))

    searcher.fit(features, labels)

    # Apply tuned parameters to the model
    adabst.set_params(**searcher.best_params_)

else:

    # Result I got when I ran the above searching
    adabst.set_params(base_estimator__max_features="sqrt",
                      base_estimator__min_samples_leaf=1,
                      base_estimator__max_depth=3,learning_rate=0.01)

sys.stdout.write("Done\n")

### Task 6: Dump your classifier, dataset, and features_list so anyone can
### check your results. You do not need to change anything below, but make sure
### that the version of poi_id.py that you submit can be run on its own and
### generates the necessary .pkl files for validating your results.

sys.stdout.write("Dumping classifier and data...   ")
sys.stdout.flush()

dump_classifier_and_data(clf, my_dataset, features_list)

sys.stdout.write("Done\n")